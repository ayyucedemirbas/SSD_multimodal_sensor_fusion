{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 4732290,
          "sourceType": "datasetVersion",
          "datasetId": 2738461
        }
      ],
      "dockerImageVersionId": 30919,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayyucedemirbas/SSD_multimodal_sensor_fusion/blob/main/multimodal_sensor_fusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/aimotive/aimotive-dataset-loader.git"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T14:51:24.896325Z",
          "iopub.execute_input": "2025-03-26T14:51:24.896514Z",
          "iopub.status.idle": "2025-03-26T14:51:25.015708Z",
          "shell.execute_reply.started": "2025-03-26T14:51:24.896495Z",
          "shell.execute_reply": "2025-03-26T14:51:25.014939Z"
        },
        "id": "0RqQPKtb-Tm5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r aimotive-dataset-loader/requirements.txt"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T14:51:25.016602Z",
          "iopub.execute_input": "2025-03-26T14:51:25.016874Z",
          "iopub.status.idle": "2025-03-26T14:51:29.995094Z",
          "shell.execute_reply.started": "2025-03-26T14:51:25.016840Z",
          "shell.execute_reply": "2025-03-26T14:51:29.994036Z"
        },
        "id": "mV6MgMQN-Tm6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets accelerate"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T14:51:33.419478Z",
          "iopub.execute_input": "2025-03-26T14:51:33.419723Z",
          "iopub.status.idle": "2025-03-26T14:51:36.762029Z",
          "shell.execute_reply.started": "2025-03-26T14:51:33.419696Z",
          "shell.execute_reply": "2025-03-26T14:51:36.760856Z"
        },
        "id": "9tsHRwMg-Tm6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('aimotive-dataset-loader')\n",
        "from typing import List, Dict, Tuple"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T14:51:36.763138Z",
          "iopub.execute_input": "2025-03-26T14:51:36.763386Z",
          "iopub.status.idle": "2025-03-26T14:51:36.767297Z",
          "shell.execute_reply.started": "2025-03-26T14:51:36.763358Z",
          "shell.execute_reply": "2025-03-26T14:51:36.766470Z"
        },
        "id": "h9ZWZkzt-Tm6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import Dataset, SequentialSampler, DataLoader\n",
        "\n",
        "# Import dataset loader components\n",
        "from typing import List, Dict, Tuple\n",
        "from src.aimotive_dataset import AiMotiveDataset\n",
        "from src.data_loader import DataItem\n",
        "from src.loaders.camera_loader import CameraData\n",
        "from src.loaders.lidar_loader import LidarData\n",
        "from src.loaders.radar_loader import RadarData"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T14:51:36.769419Z",
          "iopub.execute_input": "2025-03-26T14:51:36.769618Z",
          "iopub.status.idle": "2025-03-26T14:51:42.969193Z",
          "shell.execute_reply.started": "2025-03-26T14:51:36.769600Z",
          "shell.execute_reply": "2025-03-26T14:51:42.968290Z"
        },
        "id": "9GbFl60b-Tm7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "CATEGORY_MAPPING = {'CAR': 0, 'Size_vehicle_m': 0,\n",
        "                    'TRUCK': 1, 'BUS': 1, 'TRUCK/BUS': 1, 'TRAIN': 1, 'Size_vehicle_xl': 1, 'VAN': 1,\n",
        "                    'PICKUP': 1,\n",
        "                    'MOTORCYCLE': 2, 'RIDER': 2, 'BICYCLE': 2, 'BIKE': 2, 'Two_wheel_without_rider': 2,\n",
        "                    'Rider': 2,\n",
        "                    'OTHER_RIDEABLE': 2, 'OTHER-RIDEABLE': 2,\n",
        "                    'PEDESTRIAN': 3, 'BABY_CARRIAGE': 3\n",
        "                    }"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T14:51:42.970128Z",
          "iopub.execute_input": "2025-03-26T14:51:42.970478Z",
          "iopub.status.idle": "2025-03-26T14:51:42.974633Z",
          "shell.execute_reply.started": "2025-03-26T14:51:42.970456Z",
          "shell.execute_reply": "2025-03-26T14:51:42.973724Z"
        },
        "id": "2A-M8Ltk-Tm7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def lidar_to_bev(point_cloud, x_range=(0, 70), y_range=(-40, 40), grid_size=(512, 512)):\n",
        "    \"\"\"\n",
        "    Projects a LiDAR point cloud onto a BEV grid.\n",
        "    Returns a torch.Tensor of shape (3, H, W) encoding density, height and intensity.\n",
        "    \"\"\"\n",
        "    H, W = grid_size\n",
        "    bev = np.zeros((3, H, W), dtype=np.float32)\n",
        "    x_min, x_max = x_range\n",
        "    y_min, y_max = y_range\n",
        "    if point_cloud.shape[0] == 0:\n",
        "        return torch.from_numpy(bev)\n",
        "    x_bins = np.linspace(x_min, x_max, W+1)\n",
        "    y_bins = np.linspace(y_min, y_max, H+1)\n",
        "    xs = point_cloud[:, 0]\n",
        "    ys = point_cloud[:, 1]\n",
        "    zs = point_cloud[:, 2]\n",
        "    intensities = point_cloud[:, 3]\n",
        "    ix = np.clip(np.digitize(xs, bins=x_bins) - 1, 0, W-1)\n",
        "    iy = np.clip(np.digitize(ys, bins=y_bins) - 1, 0, H-1)\n",
        "    for i in range(point_cloud.shape[0]):\n",
        "        bev[0, iy[i], ix[i]] += 1\n",
        "        bev[1, iy[i], ix[i]] = max(bev[1, iy[i], ix[i]], zs[i])\n",
        "        bev[2, iy[i], ix[i]] += intensities[i]\n",
        "    if bev[0].max() > 0:\n",
        "        bev[0] = bev[0] / bev[0].max()\n",
        "    bev[1] = np.clip(bev[1] / 3.0, 0, 1)\n",
        "    mask = bev[0] > 0\n",
        "    bev[2, mask] = bev[2, mask] / (bev[0, mask] * bev[0].max())\n",
        "    bev[2] = np.clip(bev[2], 0, 1)\n",
        "    return torch.from_numpy(bev)\n",
        "\n",
        "def radar_to_map(point_cloud, x_range=(0, 70), y_range=(-40, 40), grid_size=(512, 512)):\n",
        "    \"\"\"\n",
        "    Converts Radar point cloud into a 2D map.\n",
        "    Returns a torch.Tensor of shape (2, H, W) encoding range and velocity.\n",
        "    \"\"\"\n",
        "    H, W = grid_size\n",
        "    radar_map = np.zeros((2, H, W), dtype=np.float32)\n",
        "    if point_cloud.shape[0] == 0:\n",
        "        return torch.from_numpy(radar_map)\n",
        "    x_min, x_max = x_range\n",
        "    y_min, y_max = y_range\n",
        "    x_bins = np.linspace(x_min, x_max, W+1)\n",
        "    y_bins = np.linspace(y_min, y_max, H+1)\n",
        "    xs = point_cloud[:, 0]\n",
        "    ys = point_cloud[:, 1]\n",
        "    ranges = np.sqrt(xs**2 + ys**2)\n",
        "    velocities = point_cloud[:, 3]\n",
        "    ix = np.clip(np.digitize(xs, bins=x_bins) - 1, 0, W-1)\n",
        "    iy = np.clip(np.digitize(ys, bins=y_bins) - 1, 0, H-1)\n",
        "    for i in range(point_cloud.shape[0]):\n",
        "        radar_map[0, iy[i], ix[i]] = max(radar_map[0, iy[i], ix[i]], ranges[i])\n",
        "        radar_map[1, iy[i], ix[i]] += velocities[i]\n",
        "    radar_map[0] = np.clip(radar_map[0] / 100.0, 0, 1)\n",
        "    count = np.zeros((H, W), dtype=np.float32)\n",
        "    for i in range(point_cloud.shape[0]):\n",
        "        count[iy[i], ix[i]] += 1\n",
        "    mask = count > 0\n",
        "    radar_map[1, mask] = radar_map[1, mask] / count[mask]\n",
        "    radar_map[1] = np.clip(radar_map[1], -1, 1)\n",
        "    radar_map[1] = (radar_map[1] + 1) / 2.0\n",
        "    return torch.from_numpy(radar_map)\n",
        "\n",
        "class AiMotiveSSD_Dataset(Dataset):\n",
        "    def __init__(self, root_dir: str, train: bool = True, grid_size: Tuple[int,int]=(512,512)):\n",
        "        data_split = 'train' if train else 'val'\n",
        "        self.dataset = AiMotiveDataset(root_dir, data_split)\n",
        "        self.grid_size = grid_size\n",
        "        self.camera_transform = T.Compose([\n",
        "            T.ToTensor(),\n",
        "            T.Resize((224, 224))\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        data_item = self.dataset.data_loader[self.dataset.dataset_index[index]]\n",
        "        bev = self.prepare_lidar_data(data_item.lidar_data)\n",
        "        front_radar = self.prepare_radar_data(data_item.radar_data.front_radar)\n",
        "        back_radar = self.prepare_radar_data(data_item.radar_data.back_radar)\n",
        "        # Fuse LiDAR and radar data (7 channels: 3 from LiDAR, 2 each from front and back radar)\n",
        "        fused_sensor = torch.cat([bev, front_radar, back_radar], dim=0)\n",
        "        # Prepare camera images from the four cameras.\n",
        "        camera_images = self.prepare_camera_data(data_item.camera_data)\n",
        "        annotations = self.get_targets(data_item.annotations.objects, CATEGORY_MAPPING)\n",
        "        # Return all sensor modalities: fused BEV/radar, camera images, and annotations.\n",
        "        return fused_sensor, camera_images, annotations\n",
        "\n",
        "    def prepare_lidar_data(self, lidar_data: LidarData) -> torch.Tensor:\n",
        "        bev = lidar_to_bev(lidar_data.top_lidar.point_cloud, grid_size=self.grid_size)\n",
        "        return bev\n",
        "\n",
        "    def prepare_radar_data(self, radar_sensor) -> torch.Tensor:\n",
        "        r_map = radar_to_map(radar_sensor.point_cloud, grid_size=self.grid_size)\n",
        "        return r_map\n",
        "\n",
        "    def prepare_camera_data(self, camera_data: CameraData) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Processes camera images from front, back, left, and right cameras.\n",
        "        Returns a tensor of shape (4, C, H, W), where C is the number of channels.\n",
        "        \"\"\"\n",
        "        front_cam = self.camera_transform(camera_data.front_camera.image)\n",
        "        back_cam = self.camera_transform(camera_data.back_camera.image)\n",
        "        left_cam = self.camera_transform(camera_data.left_camera.image)\n",
        "        right_cam = self.camera_transform(camera_data.right_camera.image)\n",
        "        # Stack images along a new dimension so you have all four views.\n",
        "        return torch.stack([front_cam, back_cam, left_cam, right_cam], dim=0)\n",
        "\n",
        "    def get_targets(self, annotations: List[Dict], category_mapping: Dict[str, int]):\n",
        "        targets = []\n",
        "        for obj in annotations:\n",
        "            # Each target: [class, x, y, l, w, q_z, vel_x, vel_y]\n",
        "            x, y, _ = [obj[f'BoundingBox3D Origin {ax}'] for ax in ['X', 'Y', 'Z']]\n",
        "            l, w, _ = [obj[f'BoundingBox3D Extent {ax}'] for ax in ['X', 'Y', 'Z']]\n",
        "            vel_x, vel_y, _ = [obj[f'Relative Velocity {ax}'] for ax in ['X', 'Y', 'Z']]\n",
        "            q_z = obj[f'BoundingBox3D Orientation Quat Z']  # Using only one orientation component\n",
        "            cat = category_mapping[obj['ObjectType']]\n",
        "            targets.append(torch.tensor([cat, x, y, l, w, q_z, vel_x, vel_y], dtype=torch.float))\n",
        "        return torch.vstack(targets) if targets else torch.zeros((1,8))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T14:51:50.511060Z",
          "iopub.execute_input": "2025-03-26T14:51:50.511382Z",
          "iopub.status.idle": "2025-03-26T14:51:50.531384Z",
          "shell.execute_reply.started": "2025-03-26T14:51:50.511342Z",
          "shell.execute_reply": "2025-03-26T14:51:50.530477Z"
        },
        "id": "aJJwAezR-Tm7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class SSDDetector(nn.Module):\n",
        "    def __init__(self, in_channels=7, num_classes=4, num_default=4):\n",
        "        \"\"\"\n",
        "        in_channels: Number of input channels (7 from fused BEV+Radar maps)\n",
        "        num_classes: Number of object classes\n",
        "        num_default: Number of default boxes per feature map cell.\n",
        "        \"\"\"\n",
        "        super(SSDDetector, self).__init__()\n",
        "        self.num_default = num_default\n",
        "        self.base = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        # Detection heads on the base feature map (assumed size: 128x128 for 512x512 input)\n",
        "        self.loc_head = nn.Conv2d(64, self.num_default * 4, kernel_size=3, padding=1)\n",
        "        self.cls_head = nn.Conv2d(64, self.num_default * num_classes, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.base(x)  # (B, 64, 128, 128)\n",
        "        loc = self.loc_head(features)  # (B, num_default*4, 128, 128)\n",
        "        conf = self.cls_head(features)  # (B, num_default*num_classes, 128, 128)\n",
        "        B = x.size(0)\n",
        "        loc = loc.permute(0, 2, 3, 1).contiguous().view(B, -1, 4)\n",
        "        conf = conf.permute(0, 2, 3, 1).contiguous().view(B, -1, self.cls_head.out_channels // self.num_default)\n",
        "        return loc, conf\n",
        "\n",
        "def generate_default_boxes(feature_map_size=(128,128), stride=4):\n",
        "    \"\"\"\n",
        "    Generate default boxes for the feature map.\n",
        "    Each cell produces 4 default boxes with predefined scales and aspect ratios.\n",
        "    Boxes are in pixel coordinates (cx, cy, w, h) for an image of size 512x512.\n",
        "    \"\"\"\n",
        "    fm_h, fm_w = feature_map_size\n",
        "    default_boxes = []\n",
        "    scales = [0.1, 0.2, 0.2, 0.3]  # relative scales\n",
        "    aspect_ratios = [1.0, 2.0, 0.5, 1.0]\n",
        "    for i in range(fm_h):\n",
        "        for j in range(fm_w):\n",
        "            cx = (j + 0.5) * stride\n",
        "            cy = (i + 0.5) * stride\n",
        "            for s, ar in zip(scales, aspect_ratios):\n",
        "                w = s * 512 * np.sqrt(ar)\n",
        "                h = s * 512 / np.sqrt(ar)\n",
        "                default_boxes.append([cx, cy, w, h])\n",
        "    return torch.tensor(default_boxes)  # (num_default, 4)\n",
        "\n",
        "def convert_gt_boxes(gt, image_size=(512,512), x_range=(0,70), y_range=(-40,40)):\n",
        "    \"\"\"\n",
        "    Convert ground truth boxes from physical coordinates to BEV pixel coordinates.\n",
        "    gt: Tensor of shape (num_objects, 8) with fields [cat, x, y, l, w, q_z, vel_x, vel_y]\n",
        "    Returns:\n",
        "      - boxes: Tensor of shape (num_objects, 4) in (cx, cy, w, h) pixel coordinates.\n",
        "      - labels: Tensor of shape (num_objects,)\n",
        "    \"\"\"\n",
        "    pixel_x = (gt[:,1] - 0) / (70 - 0) * image_size[1]\n",
        "    pixel_y = (gt[:,2] - (-40)) / (80) * image_size[0]\n",
        "    pixel_w = gt[:,3] / (70) * image_size[1]\n",
        "    pixel_h = gt[:,4] / (80) * image_size[0]\n",
        "    boxes = torch.stack([pixel_x, pixel_y, pixel_w, pixel_h], dim=1)\n",
        "    labels = gt[:,0].long()\n",
        "    return boxes, labels\n",
        "\n",
        "def compute_iou(boxes1, boxes2):\n",
        "    \"\"\"\n",
        "    Compute IoU between two sets of boxes.\n",
        "    boxes: Tensor of shape (N,4) in (cx, cy, w, h) format.\n",
        "    \"\"\"\n",
        "    boxes1_x1 = boxes1[:,0] - boxes1[:,2] / 2\n",
        "    boxes1_y1 = boxes1[:,1] - boxes1[:,3] / 2\n",
        "    boxes1_x2 = boxes1[:,0] + boxes1[:,2] / 2\n",
        "    boxes1_y2 = boxes1[:,1] + boxes1[:,3] / 2\n",
        "\n",
        "    boxes2_x1 = boxes2[:,0] - boxes2[:,2] / 2\n",
        "    boxes2_y1 = boxes2[:,1] - boxes2[:,3] / 2\n",
        "    boxes2_x2 = boxes2[:,0] + boxes2[:,2] / 2\n",
        "    boxes2_y2 = boxes2[:,1] + boxes2[:,3] / 2\n",
        "\n",
        "    inter_x1 = torch.max(boxes1_x1.unsqueeze(1), boxes2_x1.unsqueeze(0))\n",
        "    inter_y1 = torch.max(boxes1_y1.unsqueeze(1), boxes2_y1.unsqueeze(0))\n",
        "    inter_x2 = torch.min(boxes1_x2.unsqueeze(1), boxes2_x2.unsqueeze(0))\n",
        "    inter_y2 = torch.min(boxes1_y2.unsqueeze(1), boxes2_y2.unsqueeze(0))\n",
        "    inter_area = torch.clamp(inter_x2 - inter_x1, min=0) * torch.clamp(inter_y2 - inter_y1, min=0)\n",
        "    area1 = (boxes1_x2 - boxes1_x1) * (boxes1_y2 - boxes1_y1)\n",
        "    area2 = (boxes2_x2 - boxes2_x1) * (boxes2_y2 - boxes2_y1)\n",
        "    union_area = area1.unsqueeze(1) + area2.unsqueeze(0) - inter_area\n",
        "    return inter_area / union_area\n",
        "\n",
        "def match_anchors(default_boxes, gt_boxes, gt_labels, iou_threshold=0.5):\n",
        "    ious = compute_iou(default_boxes, gt_boxes)  # (N_default, M)\n",
        "    best_gt_iou, best_gt_idx = ious.max(dim=1)    # For each default, best IoU and corresponding gt index.\n",
        "    cls_targets = torch.zeros(default_boxes.size(0), dtype=torch.long, device=default_boxes.device)\n",
        "    loc_targets = torch.zeros(default_boxes.size(0), 4, device=default_boxes.device)\n",
        "    pos_idx = best_gt_iou >= iou_threshold\n",
        "    if pos_idx.sum() > 0:\n",
        "        assigned_gt_boxes = gt_boxes[best_gt_idx[pos_idx]]\n",
        "        assigned_defaults = default_boxes[pos_idx]\n",
        "        offsets = torch.zeros_like(assigned_defaults)\n",
        "        offsets[:, 0] = (assigned_gt_boxes[:, 0] - assigned_defaults[:, 0]) / assigned_defaults[:, 2]\n",
        "        offsets[:, 1] = (assigned_gt_boxes[:, 1] - assigned_defaults[:, 1]) / assigned_defaults[:, 3]\n",
        "        offsets[:, 2] = torch.log(assigned_gt_boxes[:, 2] / assigned_defaults[:, 2])\n",
        "        offsets[:, 3] = torch.log(assigned_gt_boxes[:, 3] / assigned_defaults[:, 3])\n",
        "        loc_targets[pos_idx] = offsets\n",
        "        cls_targets[pos_idx] = gt_labels[best_gt_idx[pos_idx]]\n",
        "    return loc_targets, cls_targets\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T14:52:21.054217Z",
          "iopub.execute_input": "2025-03-26T14:52:21.054517Z",
          "iopub.status.idle": "2025-03-26T14:52:21.130868Z",
          "shell.execute_reply.started": "2025-03-26T14:52:21.054493Z",
          "shell.execute_reply": "2025-03-26T14:52:21.129909Z"
        },
        "id": "-3x-o5GQ-Tm7",
        "outputId": "ec81a652-6516-4fb4-d6c7-8e07afe1bf9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Using device: cuda:0\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T14:53:21.755139Z",
          "iopub.execute_input": "2025-03-26T14:53:21.755466Z",
          "iopub.status.idle": "2025-03-26T14:53:21.759376Z",
          "shell.execute_reply.started": "2025-03-26T14:53:21.755443Z",
          "shell.execute_reply": "2025-03-26T14:53:21.758523Z"
        },
        "id": "Lrdbgf53-Tm7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "root_directory = '/kaggle/input/aimotive-multimodal-dataset'\n",
        "train_dataset = AiMotiveSSD_Dataset(root_directory, train=True, grid_size=(512,512))\n",
        "train_sampler = SequentialSampler(train_dataset)\n",
        "\n",
        "def custom_collate_fn(batch):\n",
        "    # Each item in batch: (fused_sensor, camera_images, annotations)\n",
        "    fused_sensors = torch.stack([item[0] for item in batch], dim=0)\n",
        "    camera_images = torch.stack([item[1] for item in batch], dim=0)  # (B, 4, C, H, W)\n",
        "    targets = [item[2] for item in batch]\n",
        "    return fused_sensors, camera_images, targets\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, sampler=train_sampler,\n",
        "                          pin_memory=False, drop_last=True, num_workers=4,\n",
        "                          collate_fn=custom_collate_fn)\n",
        "\n",
        "model = SSDDetector(in_channels=7, num_classes=4, num_default=4)\n",
        "model = model.to(device)\n",
        "\n",
        "criterion_loc = nn.SmoothL1Loss()\n",
        "criterion_cls = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "default_boxes = generate_default_boxes(feature_map_size=(128,128), stride=4)\n",
        "default_boxes = default_boxes.to(device)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T14:58:53.444434Z",
          "iopub.execute_input": "2025-03-26T14:58:53.444741Z",
          "iopub.status.idle": "2025-03-26T14:58:54.213618Z",
          "shell.execute_reply.started": "2025-03-26T14:58:53.444717Z",
          "shell.execute_reply": "2025-03-26T14:58:54.212689Z"
        },
        "id": "E4IuX6lE-Tm8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for step, (fused_sensor, camera_images, targets) in enumerate(train_loader):\n",
        "        fused_sensor = fused_sensor.to(device)\n",
        "        camera_images = camera_images.to(device)\n",
        "        batch_loc_targets = []\n",
        "        batch_cls_targets = []\n",
        "        for gt in targets:\n",
        "            gt = gt.float()  # shape: (num_objects, 8)\n",
        "            gt_boxes, gt_labels = convert_gt_boxes(gt, image_size=(512,512), x_range=(0,70), y_range=(-40,40))\n",
        "            gt_boxes = gt_boxes.to(device)\n",
        "            gt_labels = gt_labels.to(device)\n",
        "            loc_t, cls_t = match_anchors(default_boxes, gt_boxes, gt_labels, iou_threshold=0.5)\n",
        "            batch_loc_targets.append(loc_t)\n",
        "            batch_cls_targets.append(cls_t)\n",
        "        batch_loc_targets = torch.stack(batch_loc_targets, dim=0)  # (B, num_default, 4)\n",
        "        batch_cls_targets = torch.stack(batch_cls_targets, dim=0)  # (B, num_default)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loc_preds, conf_preds = model(fused_sensor)  # loc_preds: (B, num_default, 4), conf_preds: (B, num_default, num_classes)\n",
        "        loss_loc = criterion_loc(loc_preds, batch_loc_targets)\n",
        "        loss_cls = criterion_cls(conf_preds.view(-1, conf_preds.size(-1)), batch_cls_targets.view(-1))\n",
        "        loss = loss_loc + loss_cls\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % 5 == 0:\n",
        "            print(f\"Epoch [{epoch+1}], Step [{step}], Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T14:58:55.108356Z",
          "iopub.execute_input": "2025-03-26T14:58:55.108660Z"
        },
        "id": "4mlkJ-X6-Tm8",
        "outputId": "9c19d709-6de6-4184-c68e-6cc9f1f7b94c"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch [1], Step [0], Loss: 1.3765\nEpoch [1], Step [5], Loss: 1.3409\nEpoch [1], Step [10], Loss: 1.3111\nEpoch [1], Step [15], Loss: 1.2693\nEpoch [1], Step [20], Loss: 1.2154\nEpoch [1], Step [25], Loss: 1.1543\nEpoch [1], Step [30], Loss: 1.0768\nEpoch [1], Step [35], Loss: 1.0365\nEpoch [1], Step [40], Loss: 0.9640\nEpoch [1], Step [45], Loss: 0.8412\nEpoch [1], Step [50], Loss: 0.7319\nEpoch [1], Step [55], Loss: 0.5929\nEpoch [1], Step [60], Loss: 0.4531\nEpoch [1], Step [65], Loss: 0.3334\nEpoch [1], Step [70], Loss: 0.2392\nEpoch [1], Step [75], Loss: 0.1335\nEpoch [1], Step [80], Loss: 0.0942\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!rm /kaggle/working/multimodal_sensor_fusion.pth"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-23T23:03:24.244903Z",
          "iopub.execute_input": "2025-03-23T23:03:24.245217Z",
          "iopub.status.idle": "2025-03-23T23:03:24.386944Z",
          "shell.execute_reply.started": "2025-03-23T23:03:24.245194Z",
          "shell.execute_reply": "2025-03-23T23:03:24.385850Z"
        },
        "id": "Spk1VzY--Tm8",
        "outputId": "6c5526fd-db80-48a4-95f7-354cd80cc0d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "rm: cannot remove '/kaggle/working/multimodal_sensor_fusion.pth': No such file or directory\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'multimodal_sensor_fusion.pth')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-23T23:03:32.820865Z",
          "iopub.execute_input": "2025-03-23T23:03:32.821162Z",
          "iopub.status.idle": "2025-03-23T23:03:32.828482Z",
          "shell.execute_reply.started": "2025-03-23T23:03:32.821140Z",
          "shell.execute_reply": "2025-03-23T23:03:32.827619Z"
        },
        "id": "kkrv8MIL-Tm8"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}