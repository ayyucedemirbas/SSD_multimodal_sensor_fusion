{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 4732290,
          "sourceType": "datasetVersion",
          "datasetId": 2738461
        }
      ],
      "dockerImageVersionId": 30919,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayyucedemirbas/SSD_multimodal_sensor_fusion/blob/main/multimodal_sensor_fusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/aimotive/aimotive-dataset-loader.git"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-23T21:52:56.089063Z",
          "iopub.execute_input": "2025-03-23T21:52:56.089336Z",
          "iopub.status.idle": "2025-03-23T21:53:11.591806Z",
          "shell.execute_reply.started": "2025-03-23T21:52:56.089315Z",
          "shell.execute_reply": "2025-03-23T21:53:11.590990Z"
        },
        "id": "FWrD0sCyM4A5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r aimotive-dataset-loader/requirements.txt"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-23T21:53:11.592851Z",
          "iopub.execute_input": "2025-03-23T21:53:11.593073Z",
          "iopub.status.idle": "2025-03-23T21:53:16.649357Z",
          "shell.execute_reply.started": "2025-03-23T21:53:11.593055Z",
          "shell.execute_reply": "2025-03-23T21:53:16.648373Z"
        },
        "id": "YpOEIV-WM4A6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets accelerate"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-23T21:53:16.650796Z",
          "iopub.execute_input": "2025-03-23T21:53:16.651057Z",
          "iopub.status.idle": "2025-03-23T21:53:20.136101Z",
          "shell.execute_reply.started": "2025-03-23T21:53:16.651034Z",
          "shell.execute_reply": "2025-03-23T21:53:20.135013Z"
        },
        "id": "xHpJYOHnM4A6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('aimotive-dataset-loader')\n",
        "from typing import List, Dict, Tuple"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-23T21:55:13.877628Z",
          "iopub.execute_input": "2025-03-23T21:55:13.877993Z",
          "iopub.status.idle": "2025-03-23T21:55:13.881786Z",
          "shell.execute_reply.started": "2025-03-23T21:55:13.877966Z",
          "shell.execute_reply": "2025-03-23T21:55:13.880891Z"
        },
        "id": "gTzxbDniM4A6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import Dataset, SequentialSampler, DataLoader\n",
        "\n",
        "from typing import List, Dict, Tuple\n",
        "from src.aimotive_dataset import AiMotiveDataset\n",
        "from src.data_loader import DataItem\n",
        "from src.loaders.camera_loader import CameraData\n",
        "from src.loaders.lidar_loader import LidarData\n",
        "from src.loaders.radar_loader import RadarData"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-23T22:48:59.998619Z",
          "iopub.execute_input": "2025-03-23T22:48:59.998994Z",
          "iopub.status.idle": "2025-03-23T22:49:00.004838Z",
          "shell.execute_reply.started": "2025-03-23T22:48:59.998964Z",
          "shell.execute_reply": "2025-03-23T22:49:00.003724Z"
        },
        "id": "mSmvFNQHM4A7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "CATEGORY_MAPPING = {'CAR': 0, 'Size_vehicle_m': 0,\n",
        "                    'TRUCK': 1, 'BUS': 1, 'TRUCK/BUS': 1, 'TRAIN': 1, 'Size_vehicle_xl': 1, 'VAN': 1,\n",
        "                    'PICKUP': 1,\n",
        "                    'MOTORCYCLE': 2, 'RIDER': 2, 'BICYCLE': 2, 'BIKE': 2, 'Two_wheel_without_rider': 2,\n",
        "                    'Rider': 2,\n",
        "                    'OTHER_RIDEABLE': 2, 'OTHER-RIDEABLE': 2,\n",
        "                    'PEDESTRIAN': 3, 'BABY_CARRIAGE': 3\n",
        "                    }"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-23T22:49:01.297872Z",
          "iopub.execute_input": "2025-03-23T22:49:01.298153Z",
          "iopub.status.idle": "2025-03-23T22:49:01.302612Z",
          "shell.execute_reply.started": "2025-03-23T22:49:01.298134Z",
          "shell.execute_reply": "2025-03-23T22:49:01.301818Z"
        },
        "id": "kYZOadL1M4A7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def lidar_to_bev(point_cloud, x_range=(0, 70), y_range=(-40, 40), grid_size=(512, 512)):\n",
        "    \"\"\"\n",
        "    Projects a LiDAR point cloud onto a BEV grid.\n",
        "    Returns a torch.Tensor of shape (3, H, W) encoding density, height and intensity.\n",
        "    \"\"\"\n",
        "    H, W = grid_size\n",
        "    bev = np.zeros((3, H, W), dtype=np.float32)\n",
        "    x_min, x_max = x_range\n",
        "    y_min, y_max = y_range\n",
        "    if point_cloud.shape[0] == 0:\n",
        "        return torch.from_numpy(bev)\n",
        "    x_bins = np.linspace(x_min, x_max, W+1)\n",
        "    y_bins = np.linspace(y_min, y_max, H+1)\n",
        "    xs = point_cloud[:, 0]\n",
        "    ys = point_cloud[:, 1]\n",
        "    zs = point_cloud[:, 2]\n",
        "    intensities = point_cloud[:, 3]\n",
        "    ix = np.clip(np.digitize(xs, bins=x_bins) - 1, 0, W-1)\n",
        "    iy = np.clip(np.digitize(ys, bins=y_bins) - 1, 0, H-1)\n",
        "    for i in range(point_cloud.shape[0]):\n",
        "        bev[0, iy[i], ix[i]] += 1\n",
        "        bev[1, iy[i], ix[i]] = max(bev[1, iy[i], ix[i]], zs[i])\n",
        "        bev[2, iy[i], ix[i]] += intensities[i]\n",
        "    if bev[0].max() > 0:\n",
        "        bev[0] = bev[0] / bev[0].max()\n",
        "    bev[1] = np.clip(bev[1] / 3.0, 0, 1)\n",
        "    mask = bev[0] > 0\n",
        "    bev[2, mask] = bev[2, mask] / (bev[0, mask] * bev[0].max())\n",
        "    bev[2] = np.clip(bev[2], 0, 1)\n",
        "    return torch.from_numpy(bev)\n",
        "\n",
        "def radar_to_map(point_cloud, x_range=(0, 70), y_range=(-40, 40), grid_size=(512, 512)):\n",
        "    \"\"\"\n",
        "    Converts Radar point cloud into a 2D map.\n",
        "    Returns a torch.Tensor of shape (2, H, W) encoding range and velocity.\n",
        "    \"\"\"\n",
        "    H, W = grid_size\n",
        "    radar_map = np.zeros((2, H, W), dtype=np.float32)\n",
        "    if point_cloud.shape[0] == 0:\n",
        "        return torch.from_numpy(radar_map)\n",
        "    x_min, x_max = x_range\n",
        "    y_min, y_max = y_range\n",
        "    x_bins = np.linspace(x_min, x_max, W+1)\n",
        "    y_bins = np.linspace(y_min, y_max, H+1)\n",
        "    xs = point_cloud[:, 0]\n",
        "    ys = point_cloud[:, 1]\n",
        "    ranges = np.sqrt(xs**2 + ys**2)\n",
        "    velocities = point_cloud[:, 3]  # Adjust column index as needed.\n",
        "    ix = np.clip(np.digitize(xs, bins=x_bins) - 1, 0, W-1)\n",
        "    iy = np.clip(np.digitize(ys, bins=y_bins) - 1, 0, H-1)\n",
        "    for i in range(point_cloud.shape[0]):\n",
        "        radar_map[0, iy[i], ix[i]] = max(radar_map[0, iy[i], ix[i]], ranges[i])\n",
        "        radar_map[1, iy[i], ix[i]] += velocities[i]\n",
        "    radar_map[0] = np.clip(radar_map[0] / 100.0, 0, 1)\n",
        "    count = np.zeros((H, W), dtype=np.float32)\n",
        "    for i in range(point_cloud.shape[0]):\n",
        "        count[iy[i], ix[i]] += 1\n",
        "    mask = count > 0\n",
        "    radar_map[1, mask] = radar_map[1, mask] / count[mask]\n",
        "    radar_map[1] = np.clip(radar_map[1], -1, 1)\n",
        "    radar_map[1] = (radar_map[1] + 1) / 2.0\n",
        "    return torch.from_numpy(radar_map)\n",
        "\n",
        "class AiMotiveSSD_Dataset(Dataset):\n",
        "    def __init__(self, root_dir: str, train: bool = True, grid_size: Tuple[int,int]=(512,512)):\n",
        "        data_split = 'train' if train else 'val'\n",
        "        self.dataset = AiMotiveDataset(root_dir, data_split)\n",
        "        self.grid_size = grid_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        data_item = self.dataset.data_loader[self.dataset.dataset_index[index]]\n",
        "        bev = self.prepare_lidar_data(data_item.lidar_data)\n",
        "        front_radar = self.prepare_radar_data(data_item.radar_data.front_radar)\n",
        "        back_radar = self.prepare_radar_data(data_item.radar_data.back_radar)\n",
        "        fused_sensor = torch.cat([bev, front_radar, back_radar], dim=0)\n",
        "        annotations = self.get_targets(data_item.annotations.objects, CATEGORY_MAPPING)\n",
        "        return fused_sensor, annotations\n",
        "\n",
        "    def prepare_lidar_data(self, lidar_data: LidarData) -> torch.Tensor:\n",
        "        bev = lidar_to_bev(lidar_data.top_lidar.point_cloud, grid_size=self.grid_size)\n",
        "        return bev\n",
        "\n",
        "    def prepare_radar_data(self, radar_sensor) -> torch.Tensor:\n",
        "        r_map = radar_to_map(radar_sensor.point_cloud, grid_size=self.grid_size)\n",
        "        return r_map\n",
        "\n",
        "    def get_targets(self, annotations: List[Dict], category_mapping: Dict[str, int]):\n",
        "        targets = []\n",
        "        for obj in annotations:\n",
        "            # Each target: [class, x, y, l, w, q_z, vel_x, vel_y]\n",
        "            x, y, _ = [obj[f'BoundingBox3D Origin {ax}'] for ax in ['X', 'Y', 'Z']]\n",
        "            l, w, _ = [obj[f'BoundingBox3D Extent {ax}'] for ax in ['X', 'Y', 'Z']]\n",
        "            vel_x, vel_y, _ = [obj[f'Relative Velocity {ax}'] for ax in ['X', 'Y', 'Z']]\n",
        "            q_z = obj[f'BoundingBox3D Orientation Quat Z']  # Using only one orientation component\n",
        "            cat = category_mapping[obj['ObjectType']]\n",
        "            targets.append(torch.tensor([cat, x, y, l, w, q_z, vel_x, vel_y], dtype=torch.float))\n",
        "        return torch.vstack(targets) if targets else torch.zeros((1,8))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-23T22:49:02.538643Z",
          "iopub.execute_input": "2025-03-23T22:49:02.539016Z",
          "iopub.status.idle": "2025-03-23T22:49:02.561486Z",
          "shell.execute_reply.started": "2025-03-23T22:49:02.538988Z",
          "shell.execute_reply": "2025-03-23T22:49:02.560730Z"
        },
        "id": "axmDmP_zM4A7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class SSDDetector(nn.Module):\n",
        "    def __init__(self, in_channels=7, num_classes=4, num_default=4):\n",
        "        \"\"\"\n",
        "        in_channels: Number of input channels (7 from fused BEV+Radar maps)\n",
        "        num_classes: Number of object classes\n",
        "        num_default: Number of default boxes per feature map cell.\n",
        "        \"\"\"\n",
        "        super(SSDDetector, self).__init__()\n",
        "        self.num_default = num_default\n",
        "        self.base = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        # Detection heads on the base feature map (assumed size: 128x128 for 512x512 input)\n",
        "        self.loc_head = nn.Conv2d(64, self.num_default * 4, kernel_size=3, padding=1)\n",
        "        self.cls_head = nn.Conv2d(64, self.num_default * num_classes, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.base(x)  # (B, 64, 128, 128)\n",
        "        loc = self.loc_head(features)  # (B, num_default*4, 128, 128)\n",
        "        conf = self.cls_head(features)  # (B, num_default*num_classes, 128, 128)\n",
        "        B = x.size(0)\n",
        "        loc = loc.permute(0, 2, 3, 1).contiguous().view(B, -1, 4)\n",
        "        conf = conf.permute(0, 2, 3, 1).contiguous().view(B, -1, self.cls_head.out_channels // self.num_default)\n",
        "        return loc, conf\n",
        "\n",
        "def generate_default_boxes(feature_map_size=(128,128), stride=4):\n",
        "    \"\"\"\n",
        "    Generate default boxes for the feature map.\n",
        "    Each cell produces 4 default boxes with predefined scales and aspect ratios.\n",
        "    Boxes are in pixel coordinates (cx, cy, w, h) for an image of size 512x512.\n",
        "    \"\"\"\n",
        "    fm_h, fm_w = feature_map_size\n",
        "    default_boxes = []\n",
        "    # Predefined scales and aspect ratios for the 4 default boxes\n",
        "    scales = [0.1, 0.2, 0.2, 0.3]  # relative to image size\n",
        "    aspect_ratios = [1.0, 2.0, 0.5, 1.0]\n",
        "    for i in range(fm_h):\n",
        "        for j in range(fm_w):\n",
        "            cx = (j + 0.5) * stride\n",
        "            cy = (i + 0.5) * stride\n",
        "            for s, ar in zip(scales, aspect_ratios):\n",
        "                w = s * 512 * np.sqrt(ar)\n",
        "                h = s * 512 / np.sqrt(ar)\n",
        "                default_boxes.append([cx, cy, w, h])\n",
        "    return torch.tensor(default_boxes)  # (num_default, 4)\n",
        "\n",
        "def convert_gt_boxes(gt, image_size=(512,512), x_range=(0,70), y_range=(-40,40)):\n",
        "    \"\"\"\n",
        "    Convert ground truth boxes from physical coordinates to BEV pixel coordinates.\n",
        "    gt: Tensor of shape (num_objects, 8) with fields [cat, x, y, l, w, q_z, vel_x, vel_y]\n",
        "    Returns:\n",
        "      - boxes: Tensor of shape (num_objects, 4) in (cx, cy, w, h) pixel coordinates.\n",
        "      - labels: Tensor of shape (num_objects,)\n",
        "    \"\"\"\n",
        "    # Convert center coordinates\n",
        "    pixel_x = (gt[:,1] - 0) / (70 - 0) * image_size[1]\n",
        "    pixel_y = (gt[:,2] - (-40)) / (80) * image_size[0]\n",
        "    # Convert object dimensions (assuming l corresponds to width in x direction and w to height in y direction)\n",
        "    pixel_w = gt[:,3] / (70) * image_size[1]\n",
        "    pixel_h = gt[:,4] / (80) * image_size[0]\n",
        "    boxes = torch.stack([pixel_x, pixel_y, pixel_w, pixel_h], dim=1)\n",
        "    labels = gt[:,0].long()\n",
        "    return boxes, labels\n",
        "\n",
        "def compute_iou(boxes1, boxes2):\n",
        "    \"\"\"\n",
        "    Compute IoU between two sets of boxes.\n",
        "    boxes: Tensor of shape (N,4) in (cx, cy, w, h) format.\n",
        "    \"\"\"\n",
        "    # Convert to (x1, y1, x2, y2)\n",
        "    boxes1_x1 = boxes1[:,0] - boxes1[:,2] / 2\n",
        "    boxes1_y1 = boxes1[:,1] - boxes1[:,3] / 2\n",
        "    boxes1_x2 = boxes1[:,0] + boxes1[:,2] / 2\n",
        "    boxes1_y2 = boxes1[:,1] + boxes1[:,3] / 2\n",
        "\n",
        "    boxes2_x1 = boxes2[:,0] - boxes2[:,2] / 2\n",
        "    boxes2_y1 = boxes2[:,1] - boxes2[:,3] / 2\n",
        "    boxes2_x2 = boxes2[:,0] + boxes2[:,2] / 2\n",
        "    boxes2_y2 = boxes2[:,1] + boxes2[:,3] / 2\n",
        "\n",
        "    inter_x1 = torch.max(boxes1_x1.unsqueeze(1), boxes2_x1.unsqueeze(0))\n",
        "    inter_y1 = torch.max(boxes1_y1.unsqueeze(1), boxes2_y1.unsqueeze(0))\n",
        "    inter_x2 = torch.min(boxes1_x2.unsqueeze(1), boxes2_x2.unsqueeze(0))\n",
        "    inter_y2 = torch.min(boxes1_y2.unsqueeze(1), boxes2_y2.unsqueeze(0))\n",
        "    inter_area = torch.clamp(inter_x2 - inter_x1, min=0) * torch.clamp(inter_y2 - inter_y1, min=0)\n",
        "    area1 = (boxes1_x2 - boxes1_x1) * (boxes1_y2 - boxes1_y1)\n",
        "    area2 = (boxes2_x2 - boxes2_x1) * (boxes2_y2 - boxes2_y1)\n",
        "    union_area = area1.unsqueeze(1) + area2.unsqueeze(0) - inter_area\n",
        "    return inter_area / union_area\n",
        "\n",
        "def match_anchors(default_boxes, gt_boxes, gt_labels, iou_threshold=0.5):\n",
        "    ious = compute_iou(default_boxes, gt_boxes)  # (N_default, M)\n",
        "    best_gt_iou, best_gt_idx = ious.max(dim=1)    # For each default, best IoU and corresponding gt index.\n",
        "    # Create tensors on the same device as default_boxes:\n",
        "    cls_targets = torch.zeros(default_boxes.size(0), dtype=torch.long, device=default_boxes.device)\n",
        "    loc_targets = torch.zeros(default_boxes.size(0), 4, device=default_boxes.device)\n",
        "    pos_idx = best_gt_iou >= iou_threshold\n",
        "    if pos_idx.sum() > 0:\n",
        "        assigned_gt_boxes = gt_boxes[best_gt_idx[pos_idx]]\n",
        "        assigned_defaults = default_boxes[pos_idx]\n",
        "        offsets = torch.zeros_like(assigned_defaults)\n",
        "        offsets[:, 0] = (assigned_gt_boxes[:, 0] - assigned_defaults[:, 0]) / assigned_defaults[:, 2]\n",
        "        offsets[:, 1] = (assigned_gt_boxes[:, 1] - assigned_defaults[:, 1]) / assigned_defaults[:, 3]\n",
        "        offsets[:, 2] = torch.log(assigned_gt_boxes[:, 2] / assigned_defaults[:, 2])\n",
        "        offsets[:, 3] = torch.log(assigned_gt_boxes[:, 3] / assigned_defaults[:, 3])\n",
        "        loc_targets[pos_idx] = offsets\n",
        "        cls_targets[pos_idx] = gt_labels[best_gt_idx[pos_idx]]\n",
        "    return loc_targets, cls_targets\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-23T22:49:06.181897Z",
          "iopub.execute_input": "2025-03-23T22:49:06.182185Z",
          "iopub.status.idle": "2025-03-23T22:49:06.197593Z",
          "shell.execute_reply.started": "2025-03-23T22:49:06.182165Z",
          "shell.execute_reply": "2025-03-23T22:49:06.196802Z"
        },
        "id": "NUYvx3QbM4A7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "root_directory = '/kaggle/input/aimotive-multimodal-dataset'\n",
        "train_dataset = AiMotiveSSD_Dataset(root_directory, train=True, grid_size=(512,512))\n",
        "train_sampler = SequentialSampler(train_dataset)\n",
        "\n",
        "# Use a custom collate function to keep targets as a list (since number of objects vary per sample)\n",
        "def custom_collate_fn(batch):\n",
        "    fused_sensors = torch.stack([item[0] for item in batch], dim=0)\n",
        "    targets = [item[1] for item in batch]\n",
        "    return fused_sensors, targets\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, sampler=train_sampler,\n",
        "                          pin_memory=False, drop_last=True, num_workers=4,\n",
        "                          collate_fn=custom_collate_fn)\n",
        "\n",
        "model = SSDDetector(in_channels=7, num_classes=4, num_default=4)\n",
        "model = model.to(device)\n",
        "\n",
        "criterion_loc = nn.SmoothL1Loss()\n",
        "criterion_cls = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "default_boxes = generate_default_boxes(feature_map_size=(128,128), stride=4)\n",
        "default_boxes = default_boxes.to(device)\n",
        "\n",
        "num_epochs = 10"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-23T22:49:11.222854Z",
          "iopub.execute_input": "2025-03-23T22:49:11.223161Z",
          "iopub.status.idle": "2025-03-23T22:49:11.823214Z",
          "shell.execute_reply.started": "2025-03-23T22:49:11.223139Z",
          "shell.execute_reply": "2025-03-23T22:49:11.822537Z"
        },
        "id": "QKGZfT_ZM4A7",
        "outputId": "beb4a395-6a8e-404e-c39a-d08c6330401e"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Using device: cuda:0\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "root_directory = '/kaggle/input/aimotive-multimodal-dataset'\n",
        "train_dataset = AiMotiveSSD_Dataset(root_directory, train=True, grid_size=(512,512))\n",
        "train_sampler = SequentialSampler(train_dataset)\n",
        "def custom_collate_fn(batch):\n",
        "    # Batch is a list of (fused_sensor, targets) pairs\n",
        "    fused_sensors = torch.stack([item[0] for item in batch], dim=0)\n",
        "    targets = [item[1] for item in batch]  # Keep targets as a list\n",
        "    return fused_sensors, targets\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, sampler=train_sampler,\n",
        "                          pin_memory=False, drop_last=True, num_workers=4,\n",
        "                          collate_fn=custom_collate_fn)\n",
        "\n",
        "def to_device(data, dev):\n",
        "    if isinstance(data, (list, tuple)):\n",
        "        return [to_device(x, dev) for x in data]\n",
        "    else:\n",
        "        return data.to(dev)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-23T22:49:13.763304Z",
          "iopub.execute_input": "2025-03-23T22:49:13.763595Z",
          "iopub.status.idle": "2025-03-23T22:49:13.894984Z",
          "shell.execute_reply.started": "2025-03-23T22:49:13.763575Z",
          "shell.execute_reply": "2025-03-23T22:49:13.894278Z"
        },
        "id": "skDuMieLM4A8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for step, (fused_sensor, targets) in enumerate(train_loader):\n",
        "        fused_sensor = fused_sensor.to(device)\n",
        "        batch_loc_targets = []\n",
        "        batch_cls_targets = []\n",
        "        # Process each sample individually.\n",
        "        for gt in targets:\n",
        "            gt = gt.float()  # shape: (num_objects, 8)\n",
        "            # Convert ground truth boxes to pixel coordinates.\n",
        "            gt_boxes, gt_labels = convert_gt_boxes(gt, image_size=(512,512), x_range=(0,70), y_range=(-40,40))\n",
        "            gt_boxes = gt_boxes.to(device)\n",
        "            gt_labels = gt_labels.to(device)\n",
        "            loc_t, cls_t = match_anchors(default_boxes, gt_boxes, gt_labels, iou_threshold=0.5)\n",
        "            batch_loc_targets.append(loc_t)\n",
        "            batch_cls_targets.append(cls_t)\n",
        "        batch_loc_targets = torch.stack(batch_loc_targets, dim=0)  # (B, num_default, 4)\n",
        "        batch_cls_targets = torch.stack(batch_cls_targets, dim=0)  # (B, num_default)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loc_preds, conf_preds = model(fused_sensor)  # loc_preds: (B, num_default, 4), conf_preds: (B, num_default, num_classes)\n",
        "        loss_loc = criterion_loc(loc_preds, batch_loc_targets)\n",
        "        loss_cls = criterion_cls(conf_preds.view(-1, conf_preds.size(-1)), batch_cls_targets.view(-1))\n",
        "        loss = loss_loc + loss_cls\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % 5 == 0:\n",
        "            print(f\"Epoch [{epoch+1}], Step [{step}], Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-23T22:49:25.519521Z",
          "iopub.execute_input": "2025-03-23T22:49:25.519829Z"
        },
        "id": "Gq2tjS9eM4A8",
        "outputId": "81c49620-09f6-42a2-f2eb-b5ef1f4e3b93"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch [1], Step [0], Loss: 1.3939\nEpoch [1], Step [5], Loss: 1.3625\nEpoch [1], Step [10], Loss: 1.3334\nEpoch [1], Step [15], Loss: 1.2975\nEpoch [1], Step [20], Loss: 1.2515\nEpoch [1], Step [25], Loss: 1.1958\nEpoch [1], Step [30], Loss: 1.1241\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'multimodal_sensor_fusion.pth')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-23T22:48:42.118080Z",
          "iopub.status.idle": "2025-03-23T22:48:42.118546Z",
          "shell.execute_reply": "2025-03-23T22:48:42.118355Z"
        },
        "id": "ynjiFpMTM4A8"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}